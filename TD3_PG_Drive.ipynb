{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WXu1r8qvSzWf"
      },
      "source": [
        "# Twin-Delayed DDPG"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YRzQUhuUTc0J"
      },
      "source": [
        "## Installing the packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "HAHMB0Ze8fU0"
      },
      "outputs": [],
      "source": [
        "!pip install torch #2.0.1 + cu118\n",
        "!pip install numpy #1.24.2\n",
        "!pip install matplotlib #3.6.3\n",
        "!pip install metadrive-simulator #0.3.0.1, For troubleshooting this command, please visit the Meta-Drive GitHub repository at: https://github.com/metadriverse/metadrive."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Xjm2onHdT-Av"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Ikr2p0Js8iB4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully registered the following environments: ['MetaDrive-validation-v0', 'MetaDrive-10env-v0', 'MetaDrive-100envs-v0', 'MetaDrive-1000envs-v0', 'SafeMetaDrive-validation-v0', 'SafeMetaDrive-10env-v0', 'SafeMetaDrive-100envs-v0', 'SafeMetaDrive-1000envs-v0', 'MARLTollgate-v0', 'MARLBottleneck-v0', 'MARLRoundabout-v0', 'MARLIntersection-v0', 'MARLParkingLot-v0', 'MARLMetaDrive-v0'].\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "from metadrive import MetaDriveEnv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pickle"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Y2nGdtlKVydr"
      },
      "source": [
        "## Step 1: We initialize the Experience Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "u5rW0IDB8nTO"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer(object):\n",
        "  #Initializes an ReplayBuffer.\n",
        "  def __init__(self, max_size=10e6):\n",
        "    self.storage = []\n",
        "    self.max_size = max_size\n",
        "    self.ptr = 0\n",
        "  \n",
        "  #Adds a transition to the replay buffer, If the buffer is already at its maximum size, the oldest transition is replaced.\n",
        "  def add(self, transition):\n",
        "    if len(self.storage) == self.max_size:\n",
        "      self.storage[int(self.ptr)] = transition\n",
        "      self.ptr = (self.ptr + 1) % self.max_size\n",
        "    else:\n",
        "      self.storage.append(transition)\n",
        "  \n",
        "  #Samples a batch of transitions from the replay buffer with default size of 100 transtions \n",
        "  def sample(self, batch_size=100):\n",
        "    ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
        "    #Loops on all sampled transtions dividing them into seperate compenents\n",
        "    for i in ind: \n",
        "      state, next_state, action, reward, done = self.storage[i]\n",
        "      batch_states.append(np.array(state, copy=False))\n",
        "      batch_next_states.append(np.array(next_state, copy=False))\n",
        "      batch_actions.append(np.array(action, copy=False))\n",
        "      batch_rewards.append(np.array(reward, copy=False))\n",
        "      batch_dones.append(np.array(done, copy=False))\n",
        "    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)\n",
        "  \n",
        "  # Making a save method to save a prexisting buffer.\n",
        "  def save_buffer(self, file_path):\n",
        "    with open(file_path, 'wb') as f:\n",
        "        pickle.dump(Self.storage, f)\n",
        "  \n",
        "  # Making a load method to load a prexisting buffer.\n",
        "  def load_buffer(self,file_path):\n",
        "      with open(file_path, 'rb') as f:\n",
        "          self.storage = pickle.load(f)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Jb7TTaHxWbQD"
      },
      "source": [
        "## Step 2: We build one neural network for the Actor model and one neural network for the Actor target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "4CeRW4D79HL0"
      },
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "  \n",
        "  #Initializes an Actor network.\n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim,400)\n",
        "    self.layer_2 = nn.Linear(400,300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "  \n",
        "  #Performs a forward pass through the Actor network.\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x)) \n",
        "    return x"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HRDDce8FXef7"
      },
      "source": [
        "## Step 3: We build two neural networks for the two Critic models and two neural networks for the two Critic targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "OCee7gwR9Jrs"
      },
      "outputs": [],
      "source": [
        "class Critic(nn.Module):\n",
        "  \n",
        "  #Initializes an Twin Critic networks.\n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "  \n",
        "    #Defining the first Critic neural network.\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "  \n",
        "    #Defining the second Critic neural network.\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim,400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "  \n",
        "  #Performs a forward pass through the Twin Critic networks.\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    \n",
        "    #Forward-Propagation on the first Critic Neural Network.\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    \n",
        "    #Forward-Propagation on the second Critic Neural Network.\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "  \n",
        "  #Calculates the Q-value using the first Critic network.\n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Selecting the best hardware to run on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "#Selecting the device (CPU or GPU).\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NzIDuONodenW"
      },
      "source": [
        "## Steps 4 to 15: Training Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "zzd0H1xukdKe"
      },
      "outputs": [],
      "source": [
        "#Building the whole Training Process into a class.\n",
        "\n",
        "class TD3(object):\n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "  \n",
        "  #Uses the actor network to select an action using the current state of the enviroment.\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    for it in range(iterations):\n",
        "      \n",
        "      #Step 4: We sample a batch of transitions (s, s’, a, r) from the memory.\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      #Step 5: From the next state s’, the Actor target plays the next action a’.\n",
        "      next_action = self.actor_target(next_state)\n",
        "      \n",
        "      #Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment.\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "      \n",
        "      #Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs.\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "      \n",
        "      #Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2).\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "      \n",
        "      #Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor.\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "      \n",
        "      #Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs.\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "      \n",
        "      #Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt).\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "      \n",
        "      #Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer.\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "      \n",
        "      #Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model.\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        #Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging.\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        \n",
        "        #Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging.\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "  \n",
        "  #Making a save method to save a trained model.\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "  #Making a load method to load a pre-trained model.\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ka-ZRtQvjBex"
      },
      "source": [
        "## We make a function that evaluates a static policy by calculating its average reward over 10 episodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "qabqiYdp9wDM"
      },
      "outputs": [],
      "source": [
        "def evaluate_policy(policy, num_episodes=10, initial_evaluation=False):\n",
        "    \n",
        "    total_reward = 0.0\n",
        "    total_steps = 0\n",
        "    max_number_of_steps = 1000 if initial_evaluation else 10000 # Limit the maximum number of steps during the initial evaluation to avoid potential scenarios where the new policy gets stuck and takes an excessively long time to complete\n",
        "    num_of_steps = 0\n",
        "\n",
        "    # Loop through a specified number of episodes\n",
        "    for i in range(num_episodes):\n",
        "        obs = env.reset()  # Reset the environment for a new episode\n",
        "        num_of_steps = 0\n",
        "        done = False\n",
        "        episode_reward = 0.0\n",
        "        info = {}\n",
        "\n",
        "        # Execute the policy until the episode terminates or maximum steps are reached\n",
        "        while not done and num_of_steps < max_number_of_steps:\n",
        "            action = policy.select_action(obs)  # Select an action based on the policy\n",
        "            next_obs, reward, done, info = env.step(action)  # Execute the action in the environment\n",
        "            episode_reward += reward  # Accumulate the reward for the episode\n",
        "            obs = next_obs  # Update the current observation\n",
        "            num_of_steps += 1\n",
        "            total_steps += 1\n",
        "        #Add the final episode reward \n",
        "        total_reward += episode_reward\n",
        "    \n",
        "    # Close the environment\n",
        "    env.close()  \n",
        "\n",
        "    # Calculate average reward and average steps per episode\n",
        "    avg_reward = total_reward / num_episodes\n",
        "    avg_steps = total_steps / num_episodes\n",
        "\n",
        "    # Print the evaluation results\n",
        "    print(\"---------------------------------------\")\n",
        "    print(\"Average Reward over the Evaluation Step: %f Average steps over the Evaluation Step: %f \" % (avg_reward, avg_steps))\n",
        "    print(\"---------------------------------------\")\n",
        "    return avg_reward\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gGuKmH_ijf7U"
      },
      "source": [
        "## We set the hyper-parameters for model training "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "HFj6wbAo97lk"
      },
      "outputs": [],
      "source": [
        "env_name = \"MetaDrive-100envs-v0\" # Name of a environment (set it to any Continous environment you want)\n",
        "seed = 0 # Random seed number\n",
        "start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
        "eval_freq = 1e4 # How often the evaluation step is performed (after how many timesteps)\n",
        "max_timesteps = 2e6 # Total number of iterations/timesteps\n",
        "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
        "expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n",
        "batch_size = 500 # Size of the batch\n",
        "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
        "tau = 0.005 # Target network update rate\n",
        "policy_noise = 0.5 # STD of Gaussian noise added to the actions for the exploration purposes\n",
        "noise_clip = 0.2 # Maximum value of the Gaussian noise added to the actions (policy)\n",
        "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## We create the replay buffer and load any existing replay buffers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "replayBuffer=ReplayBuffer()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Hjwf2HCol3XP"
      },
      "source": [
        "## We create a file name for the two saved models: the Actor and Critic models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "1fyH8N5z-o3o"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Settings: TD3_MetaDrive-100envs-v0_0\n",
            "---------------------------------------\n"
          ]
        }
      ],
      "source": [
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kop-C96Aml8O"
      },
      "source": [
        "## We create a folder inside which will be saved the trained models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Src07lvY-zXb"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"./results\"):\n",
        "  os.makedirs(\"./results\")\n",
        "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
        "  os.makedirs(\"./pytorch_models\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qEAzOd47mv1Z"
      },
      "source": [
        "## We create the MetaDrive environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "CyQXJUIs-6BV"
      },
      "outputs": [],
      "source": [
        "config = dict(\n",
        "    use_render=False,  #Flag to enable rendering.\n",
        "    start_seed=100,  #Starting seed value for reproducibility.\n",
        "    num_scenarios=20,  #Number of scenarios.\n",
        "    traffic_density=0.1,  #Density of traffic.\n",
        "    random_lane_width=True,  #Flag to randomly set lane widths.\n",
        "    random_lane_num=True,  #Flag to randomly set the number of lanes.\n",
        "    map=5,  #Map block size selection.\n",
        "    vehicle_config={\n",
        "        'increment_steering': False,\n",
        "        'vehicle_model': 'default',\n",
        "        'show_navi_mark': True,\n",
        "        'extra_action_dim': 0,\n",
        "        'enable_reverse': False,\n",
        "        'random_navi_mark_color': False,\n",
        "        'show_dest_mark': False,\n",
        "        'show_line_to_dest': False,\n",
        "        'show_line_to_navi_mark': False,\n",
        "        'use_special_color': False,\n",
        "        'image_source': 'rgb_camera',\n",
        "        'navigation_module': None,\n",
        "        'need_navigation': True,\n",
        "        'spawn_lane_index': ('>', '>>', 0),\n",
        "        'spawn_longitude': 5.0,\n",
        "        'spawn_lateral': 0.0,\n",
        "        'destination': None,\n",
        "        'spawn_position_heading': None,\n",
        "        'spawn_velocity': None,\n",
        "        'spawn_velocity_car_frame': False,\n",
        "        'overtake_stat': False,\n",
        "        'random_color': False,\n",
        "        'random_agent_model': False,\n",
        "        'width': None,\n",
        "        'length': None,\n",
        "        'height': None,\n",
        "        'mass': None,\n",
        "        'lidar': {\n",
        "            'num_lasers': 40, #Configuring LiDAR array with 40 evenly spread rays in a circular pattern.\n",
        "            'distance': 50,\n",
        "            'num_others': 0,\n",
        "            'gaussian_noise': 0.0,\n",
        "            'dropout_prob': 0.0,\n",
        "            'add_others_navi': False\n",
        "        },\n",
        "        'side_detector': {\n",
        "            'num_lasers': 0,\n",
        "            'distance': 50,\n",
        "            'gaussian_noise': 0.0,\n",
        "            'dropout_prob': 0.0\n",
        "        },\n",
        "        'lane_line_detector': {\n",
        "            'num_lasers': 0,\n",
        "            'distance': 20,\n",
        "            'gaussian_noise': 0.0,\n",
        "            'dropout_prob': 0.0\n",
        "        },\n",
        "        'show_lidar': True, #Flag to enable rendering of LiDAR rays for improved visualization\n",
        "        'mini_map': (84, 84, 250),\n",
        "        'rgb_camera': (84, 84),\n",
        "        'depth_camera': (84, 84, False),\n",
        "        'main_camera': (1200, 900),\n",
        "        'show_side_detector': False,\n",
        "        'show_lane_line_detector': False,\n",
        "        'rgb_clip': True,\n",
        "        'stack_size': 3,\n",
        "        'rgb_to_grayscale': False,\n",
        "        'gaussian_noise': 0.0,\n",
        "        'dropout_prob': 0.0\n",
        "    },\n",
        "    max_step_per_agent=25000,  #Maximum number of steps per agent.\n",
        "    random_traffic=True,  #Flag to enable random traffic.\n",
        "    use_lateral_reward=True,  #Flag to use lateral reward.\n",
        "    speed_reward=0.1,  #Reward value for speed.\n",
        "    success_reward=10.0,  #Reward value for successful completion.\n",
        "    out_of_road_penalty=50.0,  #Penalty for going out of road boundaries.\n",
        "    crash_vehicle_penalty=50.0,  #Penalty for crashing into other vehicles.\n",
        "    crash_object_penalty=50.0  #Penalty for crashing into objects.\n",
        ")\n",
        "env =MetaDriveEnv(config)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5YdPG4HXnNsh"
      },
      "source": [
        "## We set seeds and we get the necessary information on the states and actions in the chosen environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:BaseEngine is not launched, fail to sync seed to engine!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n"
          ]
        }
      ],
      "source": [
        "env.seed(seed)  #Set the seed for the environment.\n",
        "torch.manual_seed(seed)  #Set the seed for PyTorch.\n",
        "np.random.seed(seed)  #Set the seed for NumPy.\n",
        "state_dim = env.observation_space.shape[0]  #Get the dimensionality of the state space.\n",
        "action_dim = env.action_space.shape[0]  #Get the dimensionality of the action space.\n",
        "max_action = float(env.action_space.high[1])  #Get the maximum action value."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HWEgDAQxnbem"
      },
      "source": [
        "## We create the policy network (the Actor model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "wTVvG7F8_EWg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A TD3 model already existed and is loaded\n"
          ]
        }
      ],
      "source": [
        "policy = TD3(state_dim, action_dim, max_action)  #Create a TD3 policy with the specified dimensions.\n",
        "actor_file_path = os.path.join(\"./pytorch_models\", f\"{file_name}_actor.pth\")  #Define the file path for the actor model.\n",
        "critic_file_path = os.path.join(\"./pytorch_models\", f\"{file_name}_critic.pth\")  #Define the file path for the critic model.\n",
        "if os.path.exists(actor_file_path) and os.path.exists(critic_file_path):\n",
        "    # If both actor and critic model files exist, load the saved models.\n",
        "    policy.load(file_name, './pytorch_models/')\n",
        "    print(\"A TD3 model already existed and is loaded\")\n",
        "else:\n",
        "    print(\"A TD3 model was created\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZI60VN2Unklh"
      },
      "source": [
        "## We create the Experience Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "sd-ZsdXR_LgV"
      },
      "outputs": [],
      "source": [
        "replay_buffer = ReplayBuffer()\n",
        "if os.path.exists(\"replay_buffer.pkl\"):\n",
        "    replay_buffer.load_buffer(\"replay_buffer.pkl\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QYOpCyiDnw7s"
      },
      "source": [
        "## We define a list where all the evaluation results over 10 episodes are stored"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 405.343096 Average steps over the Evaluation Step: 539.300000 \n",
            "---------------------------------------\n"
          ]
        }
      ],
      "source": [
        "evaluations = [evaluate_policy(policy,initial_evaluation=True)]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "31n5eb03p-Fm"
      },
      "source": [
        "## We initialize the variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_episode_steps = env.config.max_step_per_agent \n",
        "timesteps_since_eval = 0\n",
        "total_timesteps = 0\n",
        "episode_num = 0\n",
        "done = True\n",
        "t0 = time.time()\n",
        "total_reward =0.0 "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q9gsjvtPqLgT"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "y_ouY4NH_Y0I"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'replay_buffer' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[20], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m total_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39mreward\n\u001b[0;32m     54\u001b[0m \u001b[39m#We store the new transition into the Experience Replay memory (ReplayBuffer).\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m replay_buffer\u001b[39m.\u001b[39madd((obs, new_obs, action, reward, done_bool))\n\u001b[0;32m     57\u001b[0m \u001b[39m#We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m obs \u001b[39m=\u001b[39m new_obs\n",
            "\u001b[1;31mNameError\u001b[0m: name 'replay_buffer' is not defined"
          ]
        }
      ],
      "source": [
        "#We start the main loop over 1000,000 timesteps.\n",
        "while total_timesteps < max_timesteps:\n",
        "  \n",
        "  #If the episode is done.\n",
        "  if done or episode_timesteps == env.config.max_step_per_agent:\n",
        "    \n",
        "    #If we are not at the very beginning, we start the training process of the model.\n",
        "    if total_timesteps != 0 and len(replay_buffer.storage) > batch_size:\n",
        "      print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n",
        "      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
        "      crash_transition=None\n",
        "    \n",
        "    #We evaluate the episode and we save the policy.\n",
        "    if timesteps_since_eval >= eval_freq:\n",
        "      timesteps_since_eval %= eval_freq\n",
        "      evaluations.append(evaluate_policy(policy))\n",
        "      policy.save(file_name, directory=\"./pytorch_models\")\n",
        "      np.save(\"./results/%s\" % (file_name), evaluations)\n",
        "\n",
        "    #When the training step is done, we reset the state of the environment.\n",
        "    obs = env.reset()\n",
        "\n",
        "    #Set the Done to False.\n",
        "    done = False\n",
        "    \n",
        "    #Set rewards and episode timesteps to zero.\n",
        "    episode_reward = 0\n",
        "    episode_timesteps = 0\n",
        "    episode_num += 1\n",
        "\n",
        "  #Random actions encourage exploration and fill replay buffer.\n",
        "  if total_timesteps < start_timesteps:\n",
        "    action = env.action_space.sample()\n",
        "    if(total_timesteps<5000):\n",
        "      action[0]=0\n",
        "  #After 10000 timesteps, we switch to the model.\n",
        "  else: \n",
        "    action = policy.select_action(np.array(obs))     \n",
        "\n",
        "    #If the explore_noise parameter is not 0, we add noise to the action and we clip it.\n",
        "    if expl_noise != 0:\n",
        "      action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
        "  \n",
        "  #The agent performs the action in the environment, then reaches the next state and receives the reward.\n",
        "  new_obs, reward, done, info = env.step(action)\n",
        "  \n",
        "  #We check if the episode is done.\n",
        "  done_bool = 0 if episode_timesteps + 1 == env.config.max_step_per_agent else float(done)\n",
        "  \n",
        "  #We increase the total reward.\n",
        "  episode_reward += reward\n",
        "  total_reward +=reward\n",
        "  \n",
        "  #We store the new transition into the Experience Replay memory (ReplayBuffer).\n",
        "  replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
        "  \n",
        "  #We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy.\n",
        "  obs = new_obs\n",
        "  episode_timesteps += 1\n",
        "  total_timesteps += 1\n",
        "  timesteps_since_eval += 1\n",
        "\n",
        "#We add the last policy evaluation to our list of evaluations and we save our model.\n",
        "evaluations.append(evaluate_policy(policy))\n",
        "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
        "np.save(\"./results/%s\" % (file_name), evaluations)\n",
        "env.reset()      \n",
        "env.close()\n",
        "#We save the replay buffer for any future training. \n",
        "replay_buffer.save_buffer(\"replay_buffer.pkl\")\n",
        "average_reward_across_episodes= total_reward/episode_num"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wi6e2-_pu05e"
      },
      "source": [
        "## Testing the adaptive Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "oW4d1YAMqif1"
      },
      "outputs": [],
      "source": [
        "def adaptive_policy_evaluation(policy, average_reward_across_episodes, num_episodes=10):\n",
        "    total_reward = 0.0\n",
        "    emotional_damage_counter = 3  # Counter for consecutive episodes with low reward\n",
        "    timesteps_since_eval = 0  # Counter for timesteps since last policy evaluation\n",
        "\n",
        "    for i in range(num_episodes):\n",
        "        obs = env.reset()  # Reset the environment for a new episode\n",
        "        num_of_steps = 0\n",
        "        done = False\n",
        "        episode_reward = 0.0\n",
        "        info = {}  # Additional information about the episode\n",
        "        episode_timesteps = 0  # Count of timesteps within the episode\n",
        "\n",
        "        while not done:\n",
        "            action = policy.select_action(obs)  # Select an action using the policy\n",
        "            new_obs, reward, done, info = env.step(action)  # Take a step in the environment\n",
        "            done_bool = 0 if episode_timesteps + 1 == env.config.max_step_per_agent else float(done)\n",
        "            # Convert done flag to a float value (0 if not done, 1 if done at the last timestep)\n",
        "            episode_reward += reward  # Accumulate the reward for the episode\n",
        "\n",
        "            replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
        "            # Store the transition in the replay buffer for training\n",
        "\n",
        "            obs = new_obs  # Update the observation for the next timestep\n",
        "            episode_timesteps += 1\n",
        "            timesteps_since_eval += 1\n",
        "            num_of_steps += 1\n",
        "\n",
        "        if timesteps_since_eval > eval_freq:\n",
        "            # Evaluate the policy after a certain number of timesteps\n",
        "            print(f\"Training model after {eval_freq} timesteps\")\n",
        "            policy.train(replay_buffer, 1000, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
        "            policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
        "        elif episode_reward < average_reward_across_episodes * 0.2 or info['crash'] or info['out_of_road']:\n",
        "            # If the episode reward is significantly below the average or the agent crashes/out of road\n",
        "            print(\"Training model due to low score\")\n",
        "            policy.train(replay_buffer, 10, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
        "            policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
        "        elif episode_reward < average_reward_across_episodes * 0.7:\n",
        "            if emotional_damage_counter == 0:\n",
        "                # If the episode reward is consistently below 70% of the average for 3 consecutive episodes\n",
        "                print(f\"Training model due to consecutive low scores ({emotional_damage_counter} episodes)\")\n",
        "                policy.train(replay_buffer, 30, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
        "                policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
        "                emotional_damage_counter = 3\n",
        "            else:\n",
        "                emotional_damage_counter -= 1\n",
        "        else:\n",
        "            emotional_damage_counter = 3\n",
        "\n",
        "        average_reward_across_episodes = (average_reward_across_episodes + episode_reward) / 2\n",
        "        total_reward += episode_reward\n",
        "        env.close()\n",
        "\n",
        "    avg_reward = total_reward / num_episodes\n",
        "    print(\"---------------------------------------\")\n",
        "    print(\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "    print(\"---------------------------------------\")\n",
        "    return avg_reward\n",
        "\n",
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")\n",
        "eval_episodes = 10\n",
        "config=dict(\n",
        "          use_render= True, \n",
        "          start_seed= 100, \n",
        "          num_scenarios=20,\n",
        "          traffic_density=0.1,\n",
        "          random_lane_width=True,\n",
        "          random_lane_num=True,\n",
        "          map=4,\n",
        "          vehicle_config= {'increment_steering': False, 'vehicle_model': 'default', 'show_navi_mark': True, 'extra_action_dim': 0, 'enable_reverse': False, 'random_navi_mark_color': False, 'show_dest_mark': False, 'show_line_to_dest': False, 'show_line_to_navi_mark': False, 'use_special_color': False, 'image_source': 'rgb_camera', 'navigation_module': None, 'need_navigation': True, 'spawn_lane_index': ('>', '>>', 0), 'spawn_longitude': 5.0, 'spawn_lateral': 0.0, 'destination': None, 'spawn_position_heading': None, 'spawn_velocity': None, 'spawn_velocity_car_frame': False, 'overtake_stat': False, 'random_color': False, 'random_agent_model': False, 'width': None, 'length': None, 'height': None, 'mass': None, 'lidar': {'num_lasers': 40, 'distance': 50, 'num_others': 0, 'gaussian_noise': 0.0, 'dropout_prob': 0.0, 'add_others_navi': False}, 'side_detector': {'num_lasers': 0, 'distance': 50, 'gaussian_noise': 0.0, 'dropout_prob': 0.0}, 'lane_line_detector': {'num_lasers': 0, 'distance': 20, 'gaussian_noise': 0.0, 'dropout_prob': 0.0}, 'show_lidar': True, 'mini_map': (84, 84, 250), 'rgb_camera': (84, 84), 'depth_camera': (84, 84, False), 'main_camera': (1200, 900), 'show_side_detector': False, 'show_lane_line_detector': False, 'rgb_clip': True, 'stack_size': 3, 'rgb_to_grayscale': False, 'gaussian_noise': 0.0, 'dropout_prob': 0.0},\n",
        "          max_step_per_agent = 25000,\n",
        "          random_traffic= True\n",
        ")\n",
        "env = MetaDriveEnv(config)\n",
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])\n",
        "policy = TD3(state_dim, action_dim, max_action)\n",
        "policy.load(file_name, './pytorch_models/')\n",
        "_ = adaptive_policy_evaluation(policy,average_reward_across_episodes=average_reward_across_episodes)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "TD3_Ant.ipynb",
      "provenance": [],
      "toc_visible": true,
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
